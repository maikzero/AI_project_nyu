# -*- coding: utf-8 -*-
"""RAG-ChatWithYourVideoLibraryAIProject-ctg7987.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vf9ldZ5Dg3BcR7S45-HBLMtB_Wwc1DSc
"""

!pip install -q datasets

!pip install --upgrade sentence-transformers==4.1.0

import transformers, sentence_transformers
print(f"transformers: {transformers.__version__}")
print(f"sentence-transformers: {sentence_transformers.__version__}")

"""Should be: transformers: 4.51.3
sentence-transformers: 4.1.0
"""

from huggingface_hub import notebook_login
notebook_login()

from datasets import load_dataset

dataset = load_dataset("aegean-ai/ai-lectures-spring-24")
print(dataset)  # Inspect the dataset structure

"""# Encode embeddings

"""

!pip install fuzzywuzzy
!pip install bertopic

import cv2
import numpy as np
from PIL import Image
import tempfile
import os
import re
from collections import OrderedDict
import spacy
from spacy import displacy
from bertopic import BERTopic

from datasets import load_dataset
import json
import os
# Create a directory to save the JSON files if it doesn't exist
os.makedirs("json_files", exist_ok=True)

# Access the train split (since that's what your dataset shows)
train_dataset = dataset['train']

# Iterate through each example and save the JSON file
for i, example in enumerate(train_dataset):
    # Get the JSON data
    json_data = example['json']

    # Define the output filename (you can customize this)
    output_filename = f"json_files/lecture_{i}.json"

    # Save the JSON data to a file
    with open(output_filename, 'w') as f:
        json.dump(json_data, f, indent=2)  # indent for pretty printing

print(f"Saved {len(train_dataset)} JSON files to the 'json_files' directory.")

sample = dataset['train']
#subtitles = sample["en.vtt"]  # Subtitles are in VTT format
metadata = sample["json"]

def advanced_clean(text):
    """Specialized cleaning for lecture transcripts"""
    # Remove filler words and artifacts
    text = re.sub(r'\b(uh|um|ah|kind of|sort of|you know)\b', '', text, flags=re.IGNORECASE)
    # Remove XML-like tags and timestamps
    text = re.sub(r'<[^>]+>', '', text)
    # Remove repeated phrases
    text = re.sub(r'(\b\w+\b)(?:\s+\1)+', r'\1', text)
    # Remove very short sentences
    if len(text.split()) < 5:
        return ""
    return text.strip()

def remove_repeated_phrases(text):
    # This regex finds repeated phrases (1 to ~15 words) and removes duplicates
    pattern = r'\b((\w+\s+){1,15})\1'
    return re.sub(pattern, r'\1', text, flags=re.IGNORECASE)

def remove_substring_sentences(sentences):
    sentences = list(set(sentences))  # Remove exact duplicates first
    filtered = []

    for s in sentences:
        if not any((s != other and s in other) for other in sentences):
            filtered.append(s)

    return filtered

def convert_to_seconds(timestamp):
    """Convert various timestamp formats to seconds (float)"""
    if isinstance(timestamp, (int, float)):
        return float(timestamp)
    elif isinstance(timestamp, str):
        if ':' in timestamp:  # Format like "00:01:30.500"
            parts = timestamp.split(':')
            if len(parts) == 3:  # HH:MM:SS.sss
                return float(parts[0]) * 3600 + float(parts[1]) * 60 + float(parts[2])
            elif len(parts) == 2:  # MM:SS.sss
                return float(parts[0]) * 60 + float(parts[1])
        return float(timestamp)  # Try direct conversion
    return 0.0  # Default fallback

nlp = spacy.load("en_core_web_sm")

def process_vtt_metadata(metadata):
    chunks = []  # Will contain lists of sentences for each video
    time_chunks = []  # Will contain timestamp information for each sentence
    all_caption_texts = []
    all_timestamps = []  # To store timestamp info for each caption

    # Loop through each video in the dataset
    for video in metadata:
        captions = video['captions']
        for caption in captions:
            all_caption_texts.append(caption['text'])
            # Store start and end times for each caption
            all_timestamps.append((caption['start'], caption['end']))

        single_line_text = " ".join(all_caption_texts)
        single_line_text = advanced_clean(single_line_text)
        single_line_text = remove_repeated_phrases(single_line_text)

        # Process with spaCy
        doc = nlp(single_line_text)

        # Get sentences and approximate their timestamps
        sentences = [sent.text for sent in doc.sents]
        sentence_timestamps = approximate_sentence_timestamps(sentences, all_timestamps, all_caption_texts)

        chunks.append(sentences)
        time_chunks.append(sentence_timestamps)

        # Reset for next video
        all_caption_texts = []
        all_timestamps = []

    return chunks, time_chunks

from fuzzywuzzy import fuzz  # or: from thefuzz import fuzz

def approximate_sentence_timestamps(sentences, caption_timestamps, caption_texts):
    """
    Approximate timestamps for sentences using fuzzy matching with original captions.
    All timestamps are converted to and handled in seconds.
    """
    sentence_times = []

    # Convert all caption timestamps to seconds upfront
    caption_timestamps_seconds = [
        (convert_to_seconds(start), convert_to_seconds(end))
        for start, end in caption_timestamps
    ]

    for sentence in sentences:
        # Find best matching caption for start of sentence
        start_match_idx, start_score = find_best_fuzzy_match(
            sentence, caption_texts, is_start=True
        )
        start_time = caption_timestamps_seconds[start_match_idx][0] if start_match_idx != -1 else None

        # Find best matching caption for end of sentence
        end_match_idx, end_score = find_best_fuzzy_match(
            sentence, caption_texts, is_start=False
        )
        end_time = caption_timestamps_seconds[end_match_idx][1] if end_match_idx != -1 else None

        # Fallback logic if no good matches found
        if start_time is None or end_time is None:
            if not sentence_times:  # First sentence
                start_time = caption_timestamps_seconds[0][0]
                end_time = caption_timestamps_seconds[0][1]
            else:
                prev_end = sentence_times[-1][1]  # Already in seconds
                avg_duration = max(3.0, len(sentence.split()) * 0.5)  # More dynamic duration estimate
                start_time = prev_end
                end_time = prev_end + avg_duration
        # Ensure start comes before end
        elif start_time >= end_time:
            end_time = start_time + max(1.0, len(sentence.split()) * 0.3)

        sentence_times.append((start_time, end_time))

    return sentence_times

def find_best_fuzzy_match(sentence, captions, is_start=True, threshold=75):
    """
    Find the best fuzzy match between a sentence and captions.
    If is_start=True, looks at the beginning of the sentence.
    If is_start=False, looks at the end of the sentence.
    """
    best_idx = -1
    best_score = 0

    # Take first/last 10 words for matching
    words = sentence.split()
    sample_size = min(10, len(words))
    query = " ".join(words[:sample_size] if is_start else words[-sample_size:])

    for i, caption in enumerate(captions):
        # Use partial ratio since we're matching parts of texts
        score = fuzz.partial_ratio(query.lower(), caption.lower())

        if score > best_score:
            best_score = score
            best_idx = i

    # Only return if above threshold
    return (best_idx, best_score) if best_score >= threshold else (-1, best_score)

"""# process Embeddings"""

a, b = process_vtt_metadata(metadata)

import pickle
# Save to file
def save_data(data, filename):
    with open(filename, 'wb') as f:  # 'wb' = write binary
        pickle.dump(data, f)

# Load from file
def load_data(filename):
    with open(filename, 'rb') as f:  # 'rb' = read binary
        return pickle.load(f)

save_data((a, b), 'caption_data.pkl')

# Load saved data
loaded_chunks, loaded_time_chunks = load_data('caption_data.pkl')

from sentence_transformers import SentenceTransformer
clip_model = 'clip-ViT-B-32'
# sentence_model = SentenceTransformer("all-MiniLM-L6-v2")
sentence_model = SentenceTransformer(clip_model)

total_models = []
all_embeddings = []  # To store embeddings for each video

for video in loaded_chunks:
    # Get embeddings first
    video_embeddings = sentence_model.encode(video, show_progress_bar=True)
    all_embeddings.append(video_embeddings)  # Store embeddings

    # Then fit BERTopic
    topic_model = BERTopic(embedding_model=sentence_model)
    topics, probs = topic_model.fit_transform(video)
    total_models.append(topic_model)
    print(topic_model.get_topic_info())

"""# Final Embeddings are located in all_embeddings
The original sentences are located in "loaded_chunks"
The time stamps for each sentence are in "loaded_time_chunks"

# Insert into Qdrant
"""

!pip install -q moviepy qdrant-client
from sentence_transformers import SentenceTransformer
from moviepy.editor import VideoFileClip
from google.colab import files
import os

from qdrant_client.models import VectorParams, Distance

from qdrant_client import QdrantClient, models

# --- Corrected Video Segment Upload Script ---

from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from uuid import uuid4
import numpy as np

# --- Start Qdrant client (in memory)
client = QdrantClient(":memory:")

collection_name = "video_captions"

# --- Load embedding model
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")  # 384 dimensions
print("Embedding model loaded.")

# --- Generate real embeddings
real_embeddings = []
for sentences in loaded_chunks:  # loaded_chunks = list of list of sentences
    sentence_embeddings = embedding_model.encode(sentences)
    real_embeddings.append(sentence_embeddings)

# --- Detect embedding dimension
embedding_dim = real_embeddings[0].shape[1]  # Correctly detect 384
print(f"Detected embedding dimension: {embedding_dim}")

# --- Recreate collection with correct size
try:
    client.get_collection(collection_name)
    print(f"Collection '{collection_name}' already exists.")
except:
    client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(
            size=embedding_dim,
            distance=Distance.COSINE,
        )
    )
    print(f"Created collection '{collection_name}' with {embedding_dim} dimensions.")

# --- Insert video segments into Qdrant
for video_idx, (sentences, embeddings, time_data) in enumerate(zip(loaded_chunks, real_embeddings, loaded_time_chunks)):
    points = []

    # Handle time_data correctly: it's a list of (start, end) pairs
    starts = [start for start, end in time_data]
    ends = [end for start, end in time_data]

    # Verify data alignment
    assert len(sentences) == len(embeddings) == len(starts) == len(ends)

    for sentence_idx, (sentence, embedding, start, end) in enumerate(zip(
        sentences,
        embeddings,
        starts,
        ends
    )):
        points.append(
            PointStruct(
                id=str(uuid4()),
                vector=embedding.tolist(),  # Convert numpy array to list
                payload={
                    "video_id": video_idx,
                    "text": sentence,
                    "position": sentence_idx,
                    "start_sec": float(start),
                    "end_sec": float(end),
                    "duration_sec": float(end) - float(start),
                }
            )
        )

    # Upload batch to Qdrant
    operation_info = client.upsert(
        collection_name=collection_name,
        points=points,
        wait=True
    )

    print(f"✅ Uploaded {len(points)} segments for Video {video_idx} "
          f"(Time range: {starts[0]:.1f}s to {ends[-1]:.1f}s)")

"""## Retrieval Processing"""

# Install Python wrapper
!pip install imageio[ffmpeg]
!pip install ffmpeg-python

# Install system ffmpeg
!apt-get update
!apt-get install ffmpeg

"""# Gradio APP using FFMPEG"""

# utils/video_utils.py
import ffmpeg
import numpy as np
from io import BytesIO

def extract_segment(video_bytes, start_sec, end_sec):
    """Extract video segment using FFmpeg without saving to disk."""
    try:
        out, _ = (
            ffmpeg.input('pipe:0')
            .trim(start=start_sec, end=end_sec)
            .setpts('PTS-STARTPTS')  # Reset timestamps
            .output('pipe:', format='mp4', vcodec='libx264')
            .run(input=video_bytes, capture_stdout=True, capture_stderr=True)
        )
        return out
    except ffmpeg.Error as e:
        print(f"FFmpeg error: {e.stderr.decode()}")
        return None

!pip install gradio

# --- HEALTH CHECK for Qdrant and Dataset ---

# Test 1: Check if Qdrant collection exists
try:
    collections = client.get_collections()
    print(f"✅ Qdrant collections found: {collections}")
except Exception as e:
    print(f"❌ Error connecting to Qdrant: {e}")

# Test 2: Try searching manually (fallback to .search())
try:
    dummy_vec = embedding_model.encode("test query").tolist()
    hits = client.search(
        collection_name=collection_name,
        query_vector=dummy_vec,
        limit=1
    )
    print(f"✅ Retrieved {len(hits)} hits from Qdrant.")
except Exception as e:
    print(f"❌ Error during Qdrant search: {e}")

# Test 3: Check if dataset has mp4 bytes
try:
    sample = dataset[0]
    if "mp4" in sample and isinstance(sample["mp4"], bytes):
        print("✅ Dataset has valid mp4 bytes.")
    else:
        print("❌ Dataset missing mp4 bytes.")
except Exception as e:
    print(f"❌ Error accessing dataset: {e}")

# --- Retrieve top subtitles given a user query ---

def retrieve_top_subtitles(question, top_k=3):
    try:
        # 1. Embed the user's question
        query_embedding = embedding_model.encode(question).tolist()

        # 2. Search in Qdrant
        hits = client.search(
            collection_name=collection_name,
            query_vector=query_embedding,
            limit=top_k
        )

        # 3. Collect the subtitles
        subtitles = []
        for hit in hits:
            if hasattr(hit, "payload") and "subtitles" in hit.payload:
                subtitle_text = hit.payload["subtitles"]
                subtitles.append(subtitle_text)

        if subtitles:
            print(f"✅ Retrieved {len(subtitles)} subtitles:")
            for i, subtitle in enumerate(subtitles, 1):
                print(f"Top {i}: {subtitle}")
        else:
            print("⚠️ No subtitles found in hits.")

        return subtitles

    except Exception as e:
        print(f"❌ Error during retrieval: {e}")
        return []

sample = dataset[0]

# Step 1: See available keys
print(f"Sample keys: {sample.keys()}")

# Step 2: See what type 'mp4' is
print(f"Type of sample['mp4']: {type(sample['mp4'])}")

# Step 3: Preview only first 100 characters/bytes
if isinstance(sample["mp4"], bytes):
    print(f"First 100 bytes: {sample['mp4'][:100]}")
elif isinstance(sample["mp4"], str):
    print(f"First 100 characters: {sample['mp4'][:100]}")
else:
    print("Unexpected format inside 'mp4' field.")

# --- Corrected Gradio App with Subtitles Summary ---

import gradio as gr
import tempfile
import os
from datasets import load_dataset  # Ensure you have datasets installed
# NOTE: Make sure you already initialized client, collection_name, embedding_model separately!

# --- Load your HuggingFace dataset ---
dataset = load_dataset("aegean-ai/ai-lectures-spring-24", split="train")

# --- Helper function ---
def extract_segment(video_bytes, start_sec, end_sec):
    temp_dir = tempfile.gettempdir()
    temp_path = os.path.join(temp_dir, f"clip_{start_sec:.2f}_{end_sec:.2f}.mp4")

    with open(temp_path, "wb") as f:
        f.write(video_bytes)

    return temp_path

# --- Main retrieval + answering function ---
def answer_with_video(question):
    # 1. Encode the question
    question_embedding = embedding_model.encode(question).tolist()

    # 2. Qdrant search (use .search because of old client version)
    hits = client.search(
        collection_name=collection_name,
        query_vector=question_embedding,
        limit=3
    )

    # --- Safety: No hits found ---
    if not hits:
        return "No relevant video segments found.", "", "", ""

    # --- Extract video clips and subtitles ---
    video_clip_paths = []
    subtitle_texts = []

    for i, hit in enumerate(hits):
        video_idx = hit.payload.get("video_id", 0)
        start = hit.payload.get("start_sec", 0)
        end = hit.payload.get("end_sec", 5)

        # Safety: check if video_idx is valid
        if video_idx >= len(dataset):
            return f"Error: Invalid video_idx {video_idx} returned.", "", "", ""

        sample = dataset[video_idx]

        # Safety: check mp4 field
        if "mp4" not in sample or not isinstance(sample["mp4"], bytes):
            return "Error: mp4 bytes not found in dataset.", "", "", ""

        video_bytes = sample["mp4"]

        if video_bytes is None:
            return "Error: No video bytes available.", "", "", ""

        # Save video bytes temporarily
        temp_dir = tempfile.gettempdir()
        clip_path = os.path.join(temp_dir, f"clip_{i}.mp4")

        with open(clip_path, "wb") as f:
            f.write(video_bytes)

        video_clip_paths.append(clip_path)

        # Collect subtitle if available
        subtitle = hit.payload.get("subtitles", "")
        if subtitle:
            subtitle_texts.append(subtitle)

    # --- Generate better answer text ---
    if subtitle_texts:
        answer_text = "Summary based on retrieved clips:\n\n" + "\n".join(
            [f"- {text}" for text in subtitle_texts]
        )
    else:
        answer_text = f"Found {len(video_clip_paths)} relevant segments, but no subtitles available."

    # --- Ensure 3 outputs ---
    while len(video_clip_paths) < 3:
        video_clip_paths.append("")

    return answer_text, video_clip_paths[0], video_clip_paths[1], video_clip_paths[2]

# --- Gradio App ---
with gr.Blocks() as demo:
    gr.Markdown("## 📚 Chat with Your Course Videos (RAG System)")

    with gr.Row():
        question = gr.Textbox(label="Ask a question about the course videos")
        submit_btn = gr.Button("Search")

    answer = gr.Textbox(label="Generated Answer")
    video1 = gr.Video(label="Relevant Clip 1")
    video2 = gr.Video(label="Relevant Clip 2")
    video3 = gr.Video(label="Relevant Clip 3")

    submit_btn.click(
        fn=answer_with_video,
        inputs=question,
        outputs=[answer, video1, video2, video3]
    )

# Launch the app
demo.launch(share=True)