{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽ¥ Demonstration: Chat with Your Course Videos (RAG System)\n",
        "\n",
        "This notebook demonstrates example questions asked to the RAG system, and displays the corresponding video clips retrieved.\n"
      ],
      "metadata": {
        "id": "0UmrOkfYwVLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video, display\n"
      ],
      "metadata": {
        "id": "V7CTHIUeweis"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 1: \"Explain how ResNets work\"\n"
      ],
      "metadata": {
        "id": "gmFd10ZfwjRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first relevant clip\n",
        "display(Video(\"clip_resnets_1.mp4\"))"
      ],
      "metadata": {
        "id": "4v7zbhJvwuqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "- The clip shows the concept of skip connections in deep neural networks.\n",
        "- It highlights how ResNets solve the vanishing gradient problem by allowing gradient flow through identity mappings.\n"
      ],
      "metadata": {
        "id": "RRuD5HHnwwlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example 2: \"How do activation functions work?\"\n"
      ],
      "metadata": {
        "id": "1RQWW37jw2WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the second relevant clip\n",
        "display(Video(\"clip_activation_1.mp4\"))"
      ],
      "metadata": {
        "id": "j0-yLtT1xS4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "- The clip explains how activation functions like ReLU introduce non-linearity into neural networks, enabling them to model complex patterns.\n"
      ],
      "metadata": {
        "id": "UOmUtCA3xYFQ"
      }
    }
  ]
}